---
title: Machine Precision Derivatives
author: Keith A. Lewis
copyright: Â© 2019
classoption: fleqn
abstract: |
	This short note demonstrates a method for computing mixed partial derivatives to machine precision.
...

Suppose there were a "number" $\epsilon$ such that $\epsilon\not=0$ but
$\epsilon^2 = 0$. If $f$ is differentiable at $x$ then, using the Taylor
series expansion,
$$
	f(x + \epsilon) = f(x) + f'(x) \epsilon.
$$
For example, if $f(x) = x^2$ then $(x + \epsilon)^2 = x^2 + 2x\epsilon$
so $f'(x) = 2x$.

There is such a "number", the $2\times 2$ matrix $\epsilon =
\begin{bmatrix}0&1\\0&0\end{bmatrix}$.  No need to compute limits of
difference quotients\cite{?} or drag in Automatic Differentiation\cite{?} machinery.

It is possible to compute arbitrary derivatives to machine precision, including
mixed partial derivatives, using this method.

## Functional Calculus

Functions on numbers can be extended to functions on linear operators
using a _functional calculus_. If $T\colon V\to V$ is a linear operator
on the vector space $V$ and $p$ is a polynomial, then $p(T)$ can be
defined in the obvious way. If $q$ is a polynomial and $q(T)$ is
invertible, then we can define $(p/q)(T) = p(T)q(T)^{-1}$ for
appropriate rational functons.

If $V$ is a Banach space we can use power series.  If $f$ is sufficiently
differentiable, $f(x) = \sum_{n\ge0} f^{(n)}(0) x^n/n!$ when $\|T\|$
is less than the radius of convergence of the series.

Similar formulas can be used to calculate derivatives to machine precision
for multivariate functions.

## Univariate Derivatives

$$
	f(x + \epsilon) = \sum_{k\ge0} \frac{D^k f(x)}{k!}\epsilon^k  = f(x) + f'(x)\epsilon,
$$

Let $\epsilon_n$ be the $n\times n$ matrix having $i,j$ entry
$\delta_{i+1,j}$, where $\delta_{i,j}$ is the Kronecker delta function
$\delta_{i,j} = 1$ if $i = j$ and 0 if not.  Using
the definition of matrix multiplication, $\epsilon^k$ has $i,j$ entry
$\delta_{i+k,j}$, so $(\epsilon_n^k)$ are linearly independent for $0\le
k < n$ and $\epsilon^k=0$ for $k\ge n$. Using the functional calculus
we can extend $f$ to a function $f\colon\mathbf{R}^n\to\mathbf{R}^n$.
Since $f(xI + \epsilon_n) = \sum_{k<n} f^{(k)}(x) \epsilon_n^k/k!$
we can calculate the derivatives of $f$ to order $n$.

## Multivariate Derivatives

It is also possible to compute mixed derivatives to machine precision.
Suppose $f\colon\mathbf{R}^n\to\mathbf{R}$. The Taylor series expansion is
$$
	f(x + \epsilon) = \sum_{k\ge0} \sum_{|\alpha|=k} \frac{D^\alpha f(x)}{\alpha!}\epsilon^\alpha,
$$
where $x = (x_1,\ldots,x_n)$,
$\epsilon = (\epsilon_1,\ldots,\epsilon_n$),
$\alpha = (\alpha_1, \ldots, \alpha_n)$,
$|\alpha| = \alpha_1 + \cdots + \alpha_n$,
$D^\alpha f(x) = \partial^{|\alpha|}f(x_1,\ldots,x_n)/\partial^{\alpha_1} x_1\cdots\partial^{\alpha_n} x_n$,
$\alpha! = \alpha_1!\cdots\alpha_n!$,
and $\epsilon^\alpha = \epsilon_1^{\alpha_1}\cdots\epsilon_n^{\alpha_n}$,
a triumph of mathematical notation if there ever was one.

## Computer Implementaton

Map $f(xI + \epsilon)$ to $(f^{(n)}(x))$.

Most computer languages have facilites for matrix multiplication
but the special structure of our operators allow for a simple
implementation. The (Toeplitz \cite{?}) algebra generated by $\epsilon_n$ is $\{\sum_{k <
n} a_k \epsilon_n^k: a_k\in\mathbf{R}\}$ which can be represented by
a vector $(a_0,\ldots,a_{n-1})$. Vector addition and subtraction work
as usual.  The product of $a = (a_k)$ and $b = (b_k)$ is $(c_k)$ where
$c_k = \sum_{i + j = k} a_i b_j$. The quotient of $a$ and $b$ is $c$
where $a = bc$ so $a_0 = b_0 c_0$, $a_1 =
b_0c_1 + b_1 c_0$, \ldots. This has the solution
$c_0 = a_0/b_0$, $c_1 = (a_1 - b_1c_0)/b_0$, \ldots.

In languages with generic \cite{?} functions we can...

$\exp(\epsilon) = \sum_n \epsilon^n/n!$

$\exp(\sum_k a_k\epsilon^k) = \sum_n (\sum_k a_k\epsilon^k)^n/n!$

## Remarks

If $f$ is _analytic_ in a neighborhood of $x$ and ...


## NOTES

Apache Java math project and cite...

http://commons.apache.org/proper/commons-math/

http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math4/analysis/differentiation/DerivativeStructure.html

Computer implemtation only involve $f(xI + \epsilon)$. The rest is taken care of by Toeplitz matrix mulitplication.
